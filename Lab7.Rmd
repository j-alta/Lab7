---
title: "Lab7"
output: pdf_document
date: "2023-04-05"
---

```{r setup, include=FALSE}
library(tidyverse)
library(haven)
library(sandwich)
library(rdrobust)
library(rpart)
library(randomForest)
training <- read_dta("atlas_training.dta")
lockbox <- read_dta("atlas_lockbox.dta")
view(training)
view(lockbox)

```

```{r q1}
#Demonstrating convergence in incomes across racial groups using all-race/gender model
#set seed
HUID <- 21519588 
set.seed(HUID)

#Demonstrating model across two generations
#Gen 1-2
parents_rank <- 57.9
kids_rank <- 33.31 + 0.351 * parents_rank
kids_rank

#Gen 2-3
parents_rank = kids_rank
kids_rank = 33.31 + 0.351 * parents_rank
kids_rank

#Iterating across multiple generations
generations <- seq(1,7,1)

parents_rank_white = 57.9
parents_rank_black = 32.7

#white gen for loop
for(i in generations){ 
  kids_rank <- 33.31 + 0.351 * parents_rank_white
  print(paste0("In generation ", i, ", parent_rank = ", parents_rank_white, ", child_rank = ", kids_rank))
  parents_rank_white <- kids_rank
}

#black gen for loop
for(i in generations){ 
  kids_rank <- 33.31 + 0.351 * parents_rank_black
  print(paste0("In generation ", i, ", parent_rank = ", parents_rank_black, ", child_rank = ", kids_rank))
  parents_rank_black <- kids_rank
}




```
Using the all-race/gender model, white and black inter-generational mobility outcomes converge around gen 7 at a rank of about 51.3. But we know that this is incorrect; let's find the steady state prediction for Black and Hispanic children using their respective rank-rank models:

```{r q1v2}

#Steady state for Black children
generations <- seq(1,7,1)
parents_rank_black = 32.7

#black gen for loop
for(i in generations){ 
  kids_rank <- 25.4 + 0.28 * parents_rank_black
  print(paste0("In generation ", i, ", parent_rank = ", parents_rank_black, ", child_rank = ", kids_rank))
  parents_rank_black <- kids_rank
}

#Steady state for Hispanic children
parents_rank_hisp = 36.17
for(i in generations){ 
  kids_rank <- 36.14 + 0.26 * parents_rank_hisp
  print(paste0("In generation ", i, ", parent_rank = ", parents_rank_hisp, ", child_rank = ", kids_rank))
  parents_rank_hisp <- kids_rank
}

```

The steady state prediction for Black children is around 35.27 and for Hispanic children, 48.83.

***Question 2***
Cross-validation helps us avoid the overfit problem by addressing the bias-variance tradeoff in machine learning models. More complex models will eventually fit the noise of the training data, which causes the overfit problem. Cross-validation addresses that by evaluating a model's performance with different sets of training data taken from the original dataset. We can cross-validate a portion of the training data to find the optimal model complexity that minimizes RMSPE and over-fitting. 

```{r q3}

#Implementing 5-fold cross-validation using two predictors

#Store predictor variables which all start with P_*
vars <- colnames(training[,grep("^[P_]", names(training))])
vars

#Create a training data frame with just predictors P_* and kfr_pooled_pooled_p25
training_subset <- subset(training, training==1, vars)
training_subset$kfr_pooled_pooled_p25 <- training[training$training==1,]$kfr_pooled_pooled_p25

#cross-validation
n <- nrow(training_subset)
K <- 5 
B <- seq(1,20,1) 

cv <- training_subset
cv$foldid <- rep(1:K,each=ceiling(n/K))[sample(1:n)]
OOS <- data.frame(fold=rep(NA,K*length(B) ), 
                  squarederror=rep(NA,K*length(B) ), 
                  maxdepth=rep(NA,K*length(B) )) 

row <- 0

for(i in B){ 
  
  for(k in 1:K){ 
    
    row <- row + 1
    
    cvtrain <- subset(cv, foldid != k) 
    
    cvfold <- subset(cv, foldid == k) 
    
    cvtree <- rpart(kfr_pooled_pooled_p25 ~ P_12 + P_80,
                    data=cvtrain,
                    maxdepth = c(i), 
                    cp=0) 
    
    
    predfull <- predict(cvtree, newdata=cvfold) 
    
    OOS$squarederror[row] <- sum((cvfold$kfr_pooled_pooled_p25 - predfull)^2) 
    
    OOS$maxdepth[row] <- i
    
    OOS$fold[row] <- k 
    
  }
  
}

OOS
summary(OOS)

ssr <- tapply(OOS$squarederror, OOS$maxdepth, sum)
ssr <- as.data.frame(ssr)
ssr$maxdepth <- seq(1,20,1)
ssr

ssr$rmse <- sqrt(ssr$ssr / nrow(training))

ggplot(ssr, aes(x=maxdepth,y=rmse)) +
  geom_point() +
  geom_line() +
  labs(y = "Cross Validation RMSPE",
       x = "Tree Depth")

cv_optimal_depth = ssr$maxdepth[which.min(ssr$rmse)]
cv_optimal_depth


```

***Question 3b***
The optimal tree depth for this training dataset is 5

***Question 3c***
I am using the following two predictors: P_12 (Total Violent and Property Crimes Rate) and P_80 (Percent of Children Eligible for Free Lunch
(Persons < 18 Years)). 

```{r q3v2}

#Using full training dataset to estimate tree of depth 5 

tree <- rpart(kfr_pooled_pooled_p25 ~ P_23 + P_80, 
                      data=training_subset, 
                      maxdepth = cv_optimal_depth, 
                      cp=0) 

#visualize tree
plot(tree, margin = 0.2)
text(tree, cex = 0.5)

#Calculate predictions for all rows in training sample
y_train_predictions_tree <- predict(tree, newdata=training_subset)

```

***Question 4***
Random forests improve upon decision trees in two distinct ways. First, they apply bagging to build a series of trees which are each trained on a subset of the original data. The average of each series' RMPSE is taken to determine the most accurate prediction model. Bagging averages across a large number of  trees to cancel out the training data noise and left with real signal instruction. The other way is through input randomization. This reduces the correlation between trees, which improves model accuracy.   

```{r q5}

#Question 5
#Random forest with at least 1000 trees bootsrap with P_12 and P_80
smallforest <- randomForest(kfr_pooled_pooled_p25 ~ P_12 + P_80, 
                               ntree=1000, 
                               mtry=2,
                               data=training_subset)
smallforest

y_train_predictions_smallforest <- predict(smallforest, newdata=training_subset, type="response")


```

```{r q6}

#Question 6
#Random forest with at least 1000 trees bootstrap with all predictor variables
mobilityforest <- randomForest(kfr_pooled_pooled_p25 ~ ., 
                               ntree=1000, 
                               mtry=40,
                               importance=TRUE, 
                               data=training_subset)
mobilityforest

y_train_predictions_forest  <- predict(mobilityforest, newdata=training_subset, type="response")


```
```{r q7}

#Determing the importance of each predictor

importance(mobilityforest)
varImpPlot(mobilityforest, type=1) 

#type	is either 1 or 2, specifying the type of importance measure 
#(1=mean decrease in accuracy, 2=mean decrease in node impurity)

as.data.frame(importance(mobilityforest)) %>%
arrange(desc(`%IncMSE`)) %>%
head(10)

```

The most important predictors are P_56 (Mentally Unhealthy Days per Month (Persons 18 Years and Over)), P_37 (black share of the population in 2000), and P_85 (percentage of the population Roman Catholic)

```{r q8}
#Question 8
#Dtermining the best model by RMSPE for each of the three models

p <- 3
RMSPE <- matrix(0, p, 1)
RMSPE[1] <- sqrt(mean((training_subset$kfr_pooled_pooled_p25 - y_train_predictions_tree)^2, na.rm=TRUE))
RMSPE[2] <- sqrt(mean((training_subset$kfr_pooled_pooled_p25 - y_train_predictions_smallforest)^2, na.rm=TRUE))
RMSPE[3] <- sqrt(mean((training_subset$kfr_pooled_pooled_p25 - y_train_predictions_forest)^2, na.rm=TRUE))

#Display a table of the results
data.frame(RMSPE, method = c("Tree", "Small RF", "Large RF"))  


```

The large random forest model (of 1000 trees and including all 120+ predictor variables) performed the best, as it has the lowest RMSPE.

```{r q9}

# Question9
#Applying models to lockbox data - which model perfoms the best with actual social mobility data?


#Merge with truth to evaluate predictions. 
atlas <- left_join(lockbox, training , by="geoid")

#Separate test data set as a separate data frame
test <- subset(atlas, training==0)

#Get predictions for test data
y_test_predictions_tree <- predict(tree, newdata=test)
y_test_predictions_smallforest <- predict(smallforest, newdata=test, type="response")
y_test_predictions_forest  <- predict(mobilityforest, newdata=test, type="response")

#Calculate RMSPE for test data
p <- 3
OOS_RMSPE <- matrix(0, p, 1)
OOS_RMSPE[1] <- sqrt(mean((test$kfr_actual - y_test_predictions_tree)^2, na.rm=TRUE))
OOS_RMSPE[2] <- sqrt(mean((test$kfr_actual - y_test_predictions_smallforest)^2, na.rm=TRUE))
OOS_RMSPE[3] <- sqrt(mean((test$kfr_actual - y_test_predictions_forest)^2, na.rm=TRUE))

# Display table of results
data.frame(OOS_RMSPE, method = c("Tree", "Small RF", "Large RF"))  

```

Once again, the large random forest model performs the best, albeit with a higher error than when using it with the training data.



